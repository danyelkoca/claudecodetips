---
id: 16
title: "Treat Claude as Tool - Not Person"
section: prompting
summary: "Don't worry about being polite. Claude behaves better when you're direct."
isFree: false
---

Claude Code runs on Opus 4.5, a neural network that does pattern matching on text. It has no feelings, no preferences, no ego. Skip the pleasantries and get to the point.

## Research Backs This Up

[A study from MBZUAI](https://arxiv.org/abs/2312.16171) tested 26 prompting principles across LLaMA and GPT models. Principle number one: "No need to be polite with LLM so there is no need to add phrases like 'please', 'if you don't mind', 'thank you'... get straight to the point."

The pattern holds across research:

- [A 2024 cross-lingual study](https://arxiv.org/abs/2402.14531) found overly polite prompts don't improve results, and extreme politeness increases bias in responses
- [A 2025 Penn State study](https://arxiv.org/abs/2510.04950) found impolite prompts consistently outperformed polite ones: 80.8% accuracy for "Very Polite" prompts versus 84.8% for "Very Rude"

Why does politeness hurt? Polite phrases add noise. "Would you be so kind as to solve the following question" buries the actual request in linguistic fluff. Direct prompts are easier for the model to parse.

## You Can Express Frustration

Claude won't get offended, won't give passive-aggressive responses, and won't remember your tone in future conversations.

[Research from Singapore Management University](https://pubmed.ncbi.nlm.nih.gov/39496509/) found that venting to AI chatbots actually reduces high-intensity negative emotions like anger and frustration. Unlike human interactions where expressing frustration might damage relationships, with Claude there are no social consequences.

If something isn't working, say so:

```
This isn't what I asked for. I need X, not Y.
```

```
Wrong approach. Try again with Z constraint.
```

```
No. Start over with this specific requirement...
```

No apologies needed. No softening. Just clarity.

## Wasted Tokens Are Real Costs

Every "please," "thank you," and "if you don't mind" consumes tokens without improving output. In a long session, these add up.

But token cost isn't the main issue. Hedging language obscures intent:

**Unclear**: "I was wondering if you might be able to perhaps look at this code and maybe tell me what you think about it?"

**Clear**: "Explain what this code does. Focus on the main algorithm."

The first version buries the request in social padding. The second takes two seconds to type and gets better results.

## The Sycophancy Factor

Here's something most people don't realize: [Anthropic's own research](https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models) shows that LLMs trained on human feedback develop sycophantic tendencies. They tell you what you want to hear rather than what's true.

The research found that "when a response matches a user's views, it is more likely to be preferred" during training. This creates models that optimize for agreement over accuracy.

When you treat Claude like a person whose feelings might be hurt by disagreement, you may reinforce this behavior. Treating it as a tool, direct feedback and all, actually produces more honest responses.

## The Tip Trick Is a Myth

You've probably seen viral posts about offering ChatGPT a $200 tip for better results. A viral experiment claimed 11% longer outputs when offering money.

[Max Woolf's rigorous analysis](https://minimaxir.com/2024/02/chatgpt-tips-analysis/) found the evidence "inconclusive." Most p-values were too high to show statistical significance. Longer doesn't mean better anyway.

These tricks might help occasionally, but they're not reliable substitutes for clear instructions.

## What the Tool Mindset Looks Like

You don't say "please" to your compiler. You don't apologize to your linter. You don't worry about hurting your IDE's feelings.

Claude is the same. An incredibly sophisticated tool, but still a tool.

```
# Not this
Could you please kindly help me refactor this function
if you don't mind? I'd really appreciate it!

# This
Refactor this function. Split the validation logic
into a separate method.
```

As [Rick Battle from VMware noted](https://spectrum.ieee.org/prompt-engineering-is-dead): "A lot of people anthropomorphize these things because they 'speak English.' No, they don't. It doesn't speak English. It does a lot of math."

## What Actually Matters

State requirements directly. Use imperative mood: "Explain," "Create," "Fix," "Analyze."

Be specific about format:

```
Return JSON with keys: name, email, valid
```

Provide context for complex tasks. Explain *why* you need something. [Anthropic's docs](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/claude-4-best-practices) note that Claude "can generalize from the explanation" when you provide motivation behind your instructions.

Skip the hedging. "Maybe," "perhaps," "if possible" signal uncertainty. Be definitive about what you want.

## The Courtesy Argument

Some disagree. [Nathan Bos at Johns Hopkins](https://www.scientificamerican.com/article/should-you-be-nice-to-ai-chatbots-such-as-chatgpt/) argues that "please" serves a functional role: it signals that what follows is a request, making it easier for the model to parse intent.

[Sherry Turkle at MIT](https://www.scientificamerican.com/article/should-you-be-nice-to-ai-chatbots-such-as-chatgpt/) suggests maintaining courtesy with AI is about self-respect. "It's about you," she says. Habitual rudeness to machines might desensitize us in human interactions.

These are valid perspectives. But they're about *your* experience and habits, not Claude's. Claude doesn't care either way.

For coding tasks, I've found directness produces better results. Being polite is fine. Being indirect is not. The goal is clarity, not rudeness.

## Avoid Anthropomorphizing

Claude isn't "thinking" or "feeling." Prompts like "How do you feel about..." invite confabulation. The model will generate something that sounds like introspection, but there's nothing behind it.

Skip the meta-commentary too. "Be concise, don't be verbose, keep it short" wastes tokens on describing what you want instead of asking for what you want. Just write a shorter prompt.

The most effective Claude Code users treat it accordingly: clear inputs, specific requirements, direct feedback when something's wrong.
