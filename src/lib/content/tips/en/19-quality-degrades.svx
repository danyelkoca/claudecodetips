---
id: 19
title: "Quality Degrades in Long Sessions"
section: session
summary: "Context window fills up. Claude gets dumber. Know when to compact or clear."
isFree: true
---

Context window fills up. Claude gets dumber. It forgets what files it was looking at, makes mistakes you specifically corrected earlier, and starts looping on solutions you already rejected. This is not a bug. It is a constraint of how large language models work.

## Why This Happens

[Claude's 200K token context window](https://platform.claude.com/docs/en/build-with-claude/context-windows) sounds large (roughly 150,000 words), but it fills fast during active coding sessions. Every message, every file read, every tool result, every reasoning step consumes tokens from this fixed budget.

### The "Lost in the Middle" Problem

[Research from Stanford, UC Berkeley, and Samaya AI](https://arxiv.org/abs/2307.03172) demonstrated that language models do not robustly use information across long contexts. Performance follows a distinctive U-shaped curve: models perform best when relevant information appears at the very beginning (primacy bias) or the very end (recency bias), and suffer significant degradation when critical information sits in the middle.

In practical terms: Claude remembers what you said at the start of the session and what you just said. Everything in between becomes increasingly fuzzy. The same research found that GPT-3.5-Turbo's accuracy on multi-document question answering dropped below its closed-book baseline when relevant information was buried in the middle of the context. Claude exhibits similar patterns.

### Attention Mechanism Limits

The computational cost of [attention scales quadratically (O(nÂ²)) with sequence length](https://medium.com/foundation-models-deep-dive/attention-part-2-of-5-the-scaling-challenge-why-standard-attention-hits-a-wall-12212ecaa404). As your session grows, the model must attend to exponentially more token relationships. The quality of those attention patterns degrades.

Even models claiming 128K+ token support do not retain all of it meaningfully. [Recent research on "context rot"](https://research.trychroma.com/context-rot) shows that model performance degrades as input length increases, often in surprising ways. Most models become unreliable well before their advertised token limits.

## Recognizing the Symptoms

The symptoms build over time. Once you know what to look for, they are obvious.

### Instruction Amnesia

You establish a rule: "Always use TypeScript strict mode" or "Never modify the database schema directly." Claude follows it for a few turns, then suddenly stops. It reverts to default behavior as if you never gave the instruction.

### The Groundhog Day Effect

The conversation starts looping. Claude asks questions you already answered. It suggests solutions you tried and rejected earlier. You feel like you are having the same conversation repeatedly.

### Degrading Response Quality

Responses become more generic. Instead of building on previous context with specific answers, Claude gives high-level, unhelpful responses. I have seen Claude go from providing precise file-specific fixes to suggesting "you might want to check your configuration."

### File and Context Confusion

Claude forgets which files it was working on. It may attempt to re-read files it already read, or worse, make changes to the wrong files entirely. It confuses what code is new versus existing.

## The `/compact` Command

The `/compact` command summarizes your conversation to free tokens while preserving important context. Claude analyzes the conversation, identifies key information worth keeping, creates a condensed summary, and replaces old messages with that summary.

```bash
/compact                              # General compaction
/compact focus on the API changes     # Preserve specific context
/compact summarize only the auth flow
```

When you run `/compact`, Claude Code takes your conversation history and creates a summary. It then starts a new internal session with this summary preloaded. This allows the agent to retain important information without carrying forward every message.

**What gets preserved:** Recent code changes, project architecture decisions, current task objectives, established coding patterns.

**What gets condensed:** Detailed explanations no longer relevant, resolved debugging sessions, exploratory discussions without code changes.

**The tradeoff:** Claude loses detail. It may repeat earlier mistakes because the nuance that led to those corrections is now summarized away. It no longer knows exactly which files it was looking at, only a summary of what it learned from them. But this is still better than hitting the context wall mid-task.

[As of v2.0.64](https://github.com/anthropics/claude-code/blob/main/CHANGELOG.md), auto-compacting is instant.

### When to Compact vs. Clear

Use `/compact` when you want to continue the current line of work with a lighter context. Use `/clear` when Claude is going in circles or when you are switching tasks entirely. See Tip 18 for more on starting fresh sessions.

## The Auto-Compact System

Claude Code automatically compacts when approaching context limits. The exact threshold has changed over time. [Some evidence suggests](https://hyperdev.matsuoka.com/p/how-claude-code-got-better-by-protecting) recent versions trigger around 64-75% utilization rather than the historical 90%+, leaving more working memory for reasoning.

### Why the Earlier Threshold Helps

By reserving 25-50K tokens for working memory, Claude maintains higher reasoning quality throughout sessions. The downside: you get fewer total tokens of conversation history. The upside: the tokens you do have are more useful.

### Known Issues

Auto-compact is not perfect. [There have been bugs](https://github.com/anthropics/claude-code/issues/6123) where auto-compact triggered at incorrect thresholds (8-12% remaining instead of appropriate levels), causing constant interruptions. [Another bug](https://github.com/anthropics/claude-code/issues/6541) caused context management to become stuck after failed compaction attempts.

### Do Not Rely on Auto-Compact Mid-Task

Auto-compact hitting mid-task is disruptive. Claude loses the thread of what it was doing. Manual compacting at strategic breakpoints (between features, after completing a subtask) prevents this. Compact proactively; do not wait for the system to do it for you.

## Proactive Management

### Monitor at 70% Capacity

Watch the context meter in Claude Code's status bar. At 70%, it is time to act. At 80%, consider running `/compact` or wrapping up your current task. Avoid the last 20% of the context window for anything touching multiple parts of your codebase.

The status bar shows "Context left until auto-compact" but only when you are near capacity. By then, it is often too late to gracefully wrap up.

### Use the `/context` Command

The `/context` command ([introduced in v1.0.86](https://claudelog.com/faqs/what-is-context-command-in-claude-code/)) shows how tokens are allocated across system prompt, tools, MCP servers, memory files, and messages.

```bash
/context
# Output shows: token counts by category, percentage used
# MCP tools: 5.7k tokens
# Messages: 42k tokens
```

Before compacting, use `/context` to identify what is consuming space. MCP servers you are not actively using still take tokens. Disabling unused MCP servers can free significant space, sometimes avoiding the need to compact altogether.

### Subagents Preserve Main Context

[Subagents operate in isolated contexts](https://code.claude.com/docs/en/sub-agents). Delegating research, file exploration, or investigation to subagents prevents the main conversation from filling up with intermediate results. See Tip 28 for more on when and how to use them.

## Advanced Strategies

### The Document and Clear Pattern

For complex tasks spanning multiple sessions:

1. Have Claude document its plan and progress in a markdown file
2. Run `/clear` to wipe the session
3. Start a new session with instructions to read the progress document and continue

This creates durable external memory. The markdown file persists across sessions and context limits.

### Save State Before Compacting

When you notice context filling up:

1. Ask Claude to summarize current state and next steps to a markdown file
2. Run `/compact`
3. Reference the markdown file in your next prompt

This preserves nuanced context that would be lost in automatic summarization. You control what gets remembered, not Claude's summarization algorithm.

## Quick Reference

| Situation | Action |
|-----------|--------|
| Finished a feature, moving to next | `/compact` at the breakpoint |
| Claude keeps making the same mistake | `/clear` and re-prompt (see Tip 18) |
| Claude is confused or going in circles | `/clear` |
| Context at 70%+ | Proactive `/compact` |
| Complex multi-session feature | Document and Clear pattern |

## The Bottom Line

Context degradation is a constraint, not a bug. The 200K token window is large but finite. Every file read, every tool call, every message eats into it.

Compact at logical breakpoints. Do not let auto-compact surprise you mid-task. Use `/context` to see what is consuming space. Delegate exploration to subagents.

Treat context as a limited resource. The developers who thrive with Claude Code are the ones who manage it proactively.
