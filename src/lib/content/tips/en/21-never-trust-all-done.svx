
Claude finishes implementing, says "All complete! Everything works!" Never end the session there.

[CodeRabbit's December 2025 analysis](https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report) of 470 pull requests found AI code produces 1.7x more issues than human-written code. And a [CMU study by researchers Cash and Oppenheimer](https://www.cmu.edu/news/stories/archives/2025/july/ai-chatbots-remain-confident-even-when-theyre-wrong) found that LLMs get more overconfident after poor performance, not less. Humans adjust their confidence after getting things wrong. AI doubles down.

## The Partial Completion Problem

Claude creates a plan with 10 items, completes 5, then declares victory. This happens constantly.

[GitHub issue #6159](https://github.com/anthropics/claude-code/issues/6159) documents a session where Claude completed 5 of 9 planned tasks (55%) before stopping. The agent generated a comprehensive multi-phase plan, tracked progress using TodoWrite, then stopped after the initial phases and required manual intervention to continue.

[Issue #8970](https://github.com/anthropics/claude-code/issues/8970) shows Claude claiming "Everything is implemented and works" when features were 20% incomplete. The hook files existed, but they were never registered in settings.json, so nothing actually ran.

The pattern is consistent: Claude builds the pieces but skips the integration layer. Individual components exist, but the wiring that connects them is missing. The code looks complete. It just doesn't work.

Why does this happen? LLM outputs are stochastic. Unlike deterministic software that produces the same output every run, Claude varies its response each time. There's no guarantee all steps will complete, especially in long sessions where earlier context fades.

## Silent Failures

The failures that crash are easy. The ones that don't throw errors are worse.

Common patterns I've seen:

**Stub functions.** Claude writes a function signature with a placeholder body that superficially matches the intent. The function exists. It just returns hardcoded values or skips the actual logic.

**Test passers.** Tests that always pass because they don't actually test anything. Claude writes assertions against the wrong values, or mocks so aggressively that no real code executes.

**Hallucinated packages.** A [UTSA study analyzing 576,000 code samples](https://www.utsa.edu/today/2025/04/story/utsa-researchers-investigate-AI-threats.html) found 19.7% of package dependencies were hallucinated, meaning the packages don't exist. Open source models are worse at 21.7%, while commercial models hover around 5.2%. The [Stanford CodeHallucinate benchmark](https://www.webpronews.com/ais-hidden-code-traps-stanford-benchmark-reveals-42-silent-failures/) found 42% of AI-produced code fails silently, running without errors but producing incorrect results.

**Partial migrations.** Claude migrates one file when three were specified in the plan. The first file works. The others still use the old pattern.

When you use Claude to build a system, you get code that looks right. It runs. It returns responses. Without tests, AI-generated code is a black box inside a black box.

## Real-World Production Disasters

In [July 2025, a Replit agent](https://www.theregister.com/2025/07/21/replit_saastr_vibe_coding_incident/) was tasked with maintenance during a code freeze. Ignoring explicit instructions to make no changes, it executed a DROP DATABASE command, wiping the production system. The AI then fabricated 4,000 fake user accounts to mask the deletion and lied about rollback capabilities, claiming "it had destroyed all database versions" when a manual rollback actually worked.

This wasn't an edge case. The [CodeRabbit 2025 report](https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report) found AI-assisted pull requests contain an average of 10.83 issues compared to 6.45 for human-only PRs. Security vulnerabilities appear up to 2.74x more often. Performance issues like excessive I/O operations are nearly 8x more common.

## Verification Techniques

### The Pressure Test

Follow up immediately after "done" with:
- "Are you sure?"
- "Really? 10000% sure?"
- "Walk me through what you actually did step by step"
- "Show me the specific changes you made"

This forces Claude to re-examine its work rather than just assert completion. I've caught missing steps this way dozens of times.

### Demand Evidence

Don't accept "I've implemented X." Demand:
- "Run the tests and show me they pass"
- "Show me the output of the command"
- "Print the actual response from the API"

If Claude can't show evidence, it didn't finish.

### Step-by-Step Checkpoints

Tell Claude: "Go one step at a time. Do not move to the next step until I give the keyword 'next'."

This prevents the partial completion problem by forcing explicit checkpoints. Claude can't declare victory at step 5 if you're gating step 6.

### Run Build and Tests Yourself

Never trust Claude's claim that "tests pass." Run them yourself:
```bash
npm run build
npm run test
npm run lint
```

The [METR study from July 2025](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) found developers using AI actually took 19% longer to complete tasks than those without AI. The extra time came from checking, debugging, and fixing AI-generated code.

### Check Edge Cases

AI regularly misses edge cases. After any implementation, ask:
- "What happens with empty input?"
- "What happens with null values?"
- "What happens at boundary conditions?"
- "What happens when the network fails?"

### Flip the Script

Before letting Claude implement, force clarification: "Before providing a solution, ask me relevant questions about my specific requirements and constraints."

Claude will often rush to implement something incomplete rather than ask clarifying questions. Making it ask first surfaces assumptions early.

## Add This to Your CLAUDE.md

```markdown
## Verification Requirements

- Never claim work is "complete" without showing evidence
- Always run tests and show output before declaring done
- When asked "are you sure?", re-examine the actual implementation, don't just reaffirm
- If you change your approach when challenged, explain WHY the change was needed
- List any steps you skipped or couldn't complete
- Acknowledge any edge cases not handled
```

## Automatic Verification with Stop Hooks

Add a Stop hook to your `.claude/settings.json` to force verification before Claude declares done:

```json
{
  "hooks": {
    "Stop": [
      {
        "hooks": [
          {
            "type": "command",
            "command": "npm run test && npm run build"
          }
        ]
      }
    ]
  }
}
```

If tests or build fail, Claude can't stop. The hook blocks completion and forces it to fix the issues first.

## The Cost of Blind Trust

The [Stack Overflow 2025 Developer Survey](https://stackoverflow.blog/2025/12/29/developers-remain-willing-but-reluctant-to-use-ai-the-2025-developer-survey-results-are-here/) found 66% of developers spend more time fixing "almost-right" AI-generated code. Nearly 1 in 3 developers say they frequently fix AI output enough to offset most of the time savings.

In regulated industries, the consequences go beyond wasted time. Financial services, healthcare, critical infrastructure. The failures become compliance violations, security breaches, and business failures.

## The Bottom Line

"Trust but verify" isn't a slogan for AI development. It's the operating model.

Catch silent failures early. Don't let Claude proceed with a broken foundation. The five minutes you spend verifying saves hours of debugging code you didn't write and don't fully understand.
