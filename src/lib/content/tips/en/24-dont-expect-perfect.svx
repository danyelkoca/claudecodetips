---
id: 24
title: "Don't Expect Perfect First Try"
section: multi-agent
summary: "Let 1 agent code, use another agent to check the code."
isFree: false
---

The same Claude that wrote buggy code cannot reliably find the bugs. This isn't a prompting problem. It's an architecture problem.

[CodeRabbit's December 2025 analysis](https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report) of 470 real-world pull requests found AI-generated code has **1.7x more issues** than human-written code. The breakdown is worse than the average suggests:

- **1.75x more** logic and correctness errors
- **2.25x more** algorithmic errors
- **1.57x more** security findings
- **1.64x more** maintainability issues

Asking Claude to review its own output doesn't fix this. It makes it worse.

## Why Self-Review Fails

When the same model generates and reviews code, confirmation bias kicks in. The initial generation creates an anchor that constrains all subsequent analysis. Claude interprets evidence in ways consistent with its own outputs. It literally cannot see its own blind spots.

[Research from Google DeepMind](https://arxiv.org/abs/2310.01798) (published at ICLR 2024) found that LLMs "struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction."

The key phrase: *without external feedback*.

Self-review in a single session provides zero external feedback. It's the same context, same reasoning patterns, same blind spots. Studies on LLM self-critique in planning tasks found that "self-critiquing appears to diminish plan generation performance" compared to systems with external verifiers.

Only [3.8% of developers](https://www.qodo.ai/reports/state-of-ai-code-quality/) report both low hallucination rates AND high confidence shipping AI code without human review. The other 96.2% have learned: verification isn't optional.

## The Generator-Critic Pattern

The fix is architectural. Separate generation from verification:

**Generator Agent**: Writes code quickly. Optimized for speed and pattern matching. Its job is to produce something that works.

**Critic Agent**: Reviews code adversarially. Fresh context, no access to the original prompt. Its job is to find problems, not validate solutions.

This separation works because the reviewer has no anchoring bias. [Organizations adopting this approach](https://www.qodo.ai/blog/why-your-ai-code-reviews-are-broken-and-how-to-fix-them/) report **40-60% improvement** in code quality metrics and **60% reduction** in post-deployment bugs.

Not from better models. From better architecture.

## How to Implement This

### Option 1: Fresh Session Review

The simplest approach. One Claude writes code, then you open a completely new session for review:

1. Agent A generates the implementation
2. Open a new terminal (completely fresh context)
3. Agent B reviews the code without seeing Agent A's reasoning
4. Iterate based on B's feedback

The reviewer never sees the original prompt, the planning, or the false starts. Just the code.

```
Review this code for bugs, security issues, and logic errors.
Focus on what's wrong, not what's right.
Be adversarial.

[paste the code]
```

### Option 2: Test-Driven Multi-Agent

[The AgentCoder framework](https://arxiv.org/abs/2312.13010) demonstrates this with three specialized agents:

- **Programmer Agent**: Generates and refines code
- **Test Designer Agent**: Creates test cases
- **Test Executor Agent**: Runs tests and provides feedback

The key insight: the test designer doesn't know how the programmer implemented the solution. It designs tests from the spec, not the implementation.

This achieves **91.5% pass rates** on benchmarks, compared to 86.8% with single-agent approaches.

### Option 3: Plan-Validate-Execute

Extend Claude's plan mode with explicit verification:

1. **Planner**: Generates implementation plan
2. **Validator**: Reviews plan (fresh session, different from planner)
3. **Executor**: Implements each step
4. **Verifier**: Confirms output matches requirements

The validator and planner must have different contexts. If the planner made a flawed assumption, the validator inheriting that context will make the same assumption.

### Option 4: Subagent Verification

[Anthropic recommends](https://www.anthropic.com/engineering/claude-code-best-practices) using subagents to "verify details or investigate particular questions, especially early on in a conversation." This preserves your main context while getting independent analysis.

In practice:

```
Verify with independent subagents that the implementation isn't overfitting to the tests.
```

Each subagent gets its own context window. No cumulative bias from the main conversation's reasoning.

## Why Fresh Context Matters

As conversations grow, Claude accumulates context that shapes its thinking. Early decisions anchor later reasoning. By 70% context utilization, quality degrades noticeably. Research on [context window degradation](https://research.trychroma.com/context-rot) shows model performance consistently drops as input length increases, with distraction effects compounding.

Separate agents avoid this. Each operates with a clean slate:

- No accumulated assumptions from earlier reasoning
- No anchoring to previous decisions
- No confirmation bias toward prior outputs

The generator can fill its context with implementation details. The reviewer starts fresh with just the code and the spec.

## Iterative Refinement That Works

[The Self-Refine pattern](https://arxiv.org/abs/2303.17651) from Carnegie Mellon shows iterative feedback improves outputs by **~20% on average**. But the key insight: feedback must come from a different evaluation context than generation.

1. Generate initial output
2. Get feedback (from separate session or subagent)
3. Refine based on feedback
4. Repeat until satisfactory

The model that generates can refine, but the feedback must come from outside.

## When Self-Critique Hurts

Self-critique without external feedback can actively degrade performance. Google's research found this is especially true for reasoning tasks. If Claude's initial answer was optimal given the prompt, asking it to second-guess itself just introduces noise.

The pattern that works: external verification for complex tasks, quick iteration for simple ones.

For a typo fix, self-review is fine. For a multi-file refactor, get a fresh set of eyes.

## Practical Rules

1. **Never let the generator review its own output in the same session**
2. **Open a new terminal for review** (not just `/clear`, start a new session)
3. **Don't share the original prompt with the reviewer** (just share the output)
4. **Run tests between generations** (automated feedback is external feedback)
5. **Commit frequently** (clean git state makes verification easier)
6. **Limit to 3-5 agents max** (beyond that, coordination overhead eats the gains)

## The Bottom Line

Perfect first-try code is a myth. Even the best models produce bugs at higher rates than human developers.

The solution isn't better prompting. It's better architecture. Let one agent code. Use another to check.

Your production systems will thank you.
