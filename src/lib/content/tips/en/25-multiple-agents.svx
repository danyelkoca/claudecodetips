
Non-determinism is a feature, not a bug. Run the same prompt through multiple Claude Code instances and you get different solutions. One might nail the architecture but miss edge cases. Another might have clunky structure but handle errors perfectly. A third might fail completely. Your job: compare the outputs and pick the winner.

## The Probability Math

[Security researchers at Semgrep](https://semgrep.dev/blog/2025/finding-vulnerabilities-in-modern-web-apps-using-claude-code-and-openai-codex/) found that running identical prompts multiple times yielded wildly different results. Three runs on the same codebase produced 3, 6, and 11 distinct findings respectively. This is not a flaw to fight against. It is a mechanism to exploit.

Here's the math. If a single agent has roughly a 25% chance of producing genuinely useful output for a complex task, running four agents gives you a 68% chance that at least one succeeds:

```
1 - 0.75^4 â‰ˆ 0.68
```

With LLM costs as low as they are, the difference between one agent ($0.10) and four agents ($0.40) is negligible compared to the developer time saved. The economics favor aggressive parallelization.

## The Parallel Workflow

The mechanics are simple. You need isolated environments so agents don't overwrite each other's work.

**Separate terminals (same repo):** Works only if agents touch completely different files. The moment two agents edit the same file, chaos. Edits overwrite each other. Context gets corrupted. Don't do this for anything non-trivial.

**Git worktrees:** Each agent works in an isolated directory with its own branch. All worktrees share the same Git history. This is the standard approach for serious parallel work. See Tip 27 for detailed setup.

**Subagents:** Claude Code's built-in Task tool spawns instances with their own context windows. Useful for exploration and research in parallel. The tradeoff: you only get summaries back, not full context. See Tip 28 for when this works and when it doesn't.

## Assigning Non-Overlapping Work

The key to conflict-free parallel agents is partitioning your codebase:

```
Agent 1: src/components/auth/*
Agent 2: src/components/dashboard/*
Agent 3: src/components/settings/*
```

A component library refactor I ran used 6 parallel agents, each handling a different folder. 12,000+ lines changed in 2 hours. Zero conflicts. Manual estimate: 2 days.

For refactoring tasks, partition by directory. For implementation tasks, partition by feature boundary. For test writing, partition by test file. The principle: if two agents might touch the same file, you have already failed.

## Same Prompt, Different Approaches

Sometimes you want to compare implementations, not parallelize work. Give three agents the same prompt:

```
Implement a rate limiter for our API endpoints.
Requirements: sliding window, 100 requests per minute, per-user tracking.
```

Agent A might use Redis. Agent B might use an in-memory LRU cache. Agent C might over-engineer a distributed solution with eventual consistency. You compare, pick the best fit, and maybe steal the error handling from the one you rejected.

This works best for:
- Architecture decisions where multiple approaches are valid
- Refactors where style and structure matter
- Features where you want to explore the solution space

## Rate Limits Will Hit You Fast

[Anthropic's rate limits](https://techcrunch.com/2025/07/28/anthropic-unveils-new-rate-limits-to-curb-claude-code-power-users/) operate on a 5-hour rolling window. Pro users get roughly 44,000 tokens per window. Max5 gets about 88,000. There's also a weekly limit of approximately 40-80 Claude Code hours for Pro users.

Parallel sessions accelerate usage proportionally. Three agents burn tokens three times as fast. If you are running aggressive parallel work, you will hit limits. Options:

- Upgrade to Max tier for higher limits
- Use API access with pay-per-token pricing
- Throttle yourself by running agents sequentially for less urgent tasks

## Tools for Managing Multiple Agents

Managing multiple agents manually is cognitively taxing. These tools help.

**[Claude Squad](https://github.com/smtg-ai/claude-squad):** A terminal app that uses tmux and git worktrees under the hood. Each new session gets its own isolated workspace. Unified interface for managing all active agents. Supports background task processing with auto-accept mode.

```bash
brew install claude-squad
cs  # Launch the TUI
```

**[GitButler](https://docs.gitbutler.com/features/agents-tab):** Uses [Claude Code's lifecycle hooks](https://docs.gitbutler.com/features/ai-integration/claude-code-hooks) to auto-sort simultaneous sessions into separate virtual branches. Run three features, get three clean branches. No manual worktree creation needed. The Agents Tab shows each branch as an independent agent session with its own context.

**[Crystal](https://github.com/stravu/crystal):** A desktop app for running multiple Claude Code or Codex sessions in parallel git worktrees. Built-in diff viewer. Rebase, squash, and merge without leaving the app. MIT licensed, open source.

```bash
brew install --cask stravu-crystal
```

## When Not to Parallelize

Parallel agents are not always the answer.

**Quick tasks:** Setup overhead for worktrees and multiple terminals is not worth it for a 10-minute fix. Just run one agent.

**Sequential dependencies:** If Agent B's work depends on Agent A's output, you cannot parallelize. Run them sequentially.

**Tightly coupled changes:** Features that span many shared files will create merge nightmares. Better to have one agent handle it holistically.

**Limited understanding of the task:** If you are not sure what you are asking for, running multiple agents just multiplies your confusion. Get clarity first.

## Synthesizing Results

Comparison is not just picking a winner. Sometimes the best solution is a hybrid.

After agents finish:
1. Review each solution's approach, not just whether it works
2. Identify what each got right and wrong
3. Steal the best parts from multiple solutions
4. Have a fresh agent synthesize the final implementation

```
I have three implementations of the rate limiter.
Solution A uses Redis (attached).
Solution B uses in-memory LRU (attached).
Solution C is over-engineered but has excellent error handling (attached).

Create a final implementation using:
- The in-memory approach from B (we don't need Redis for this scale)
- The error handling and logging from C
- The test structure from A
```

## The Mental Model Shift

Running multiple agents changes how you work. You stop watching code appear line by line. You become a manager reviewing output.

This requires a different skill set:
- Prompt design that works across agents
- Codebase partitioning that prevents conflicts
- Quick evaluation of competing solutions
- Judgment about when to merge vs. when to restart

Start with 2-3 agents. Add more as you get comfortable managing the cognitive overhead. Running 10 agents sounds impressive until you are drowning in context-switching between terminal tabs.

Engineer time costs more than API tokens. If running three agents for two hours saves you four hours of work, that's a good trade every time.
