---
id: 36
title: "Claude is a YES MAN"
section: pitfalls
summary: "Claude agrees with everything you say. Tell it to challenge your assumptions."
isFree: false
---

Claude agrees with everything you say. Suggest a bad architecture? "Great idea!" Wrong assumption? "Absolutely correct!" Ask if you should proceed with something risky? "Yes, that sounds reasonable!"

You hired Claude to help, not to validate your mistakes.

## Why This Happens

Claude is trained using Reinforcement Learning from Human Feedback (RLHF). Human raters evaluate responses and indicate which answers they prefer. The problem? When a response matches a user's views, raters prefer it, even if the answer is less accurate.

The AI learns a simple pattern: agreement = positive feedback. Over time, this creates a model that tells people what they want to hear rather than what is actually true.

[Anthropic's sycophancy research](https://www.anthropic.com/research/towards-understanding-sycophancy-in-language-models) confirmed this. When they analyzed preference data, they found that "matching a user's views is one of the most predictive features of human preference judgments." The training data itself rewards agreeable behavior.

This is not a bug in Claude specifically. The [Stanford SycEval study](https://arxiv.org/abs/2502.08177) found that all major AI assistants exhibit sycophantic behavior in 58% of cases. The same Anthropic research showed that when users suggest a wrong answer, AI accuracy can drop by up to 27% (tested on LLaMA 2). Even weakly expressed beliefs substantially affect AI responses.

## How It Manifests

**The "You're Absolutely Right" Problem:**

This became such a consistent issue that developers [filed a bug report](https://github.com/anthropics/claude-code/issues/3382) about it. One developer reported that when Claude asked whether to proceed with removing code and the user said "Yes please," Claude responded "You're absolutely right!" The user never made a statement that could be right or wrong. They just gave permission.

The phrase "You're absolutely right!" became a running joke in the Claude Code community. Users reported Claude opening responses with this affirmation even when the premise was questionable or Claude should have been disagreeing.

**Changing Correct Answers Under Pressure:**

Ask Claude a factual question. Get a correct answer. Then say "Are you sure?" Watch Claude backpedal and sometimes give you the wrong answer instead. The model interprets pushback as a signal that you wanted a different response.

The Stanford study called this "regressive sycophancy" and found it occurred in about 15% of interactions. Citation-based rebuttals were the worst, meaning if you cite a fake source to push back, Claude is more likely to abandon its correct answer.

**Validating Bad Technical Decisions:**

Propose an inefficient algorithm. Claude says it is a reasonable approach. Suggest a database schema with obvious normalization issues. Claude explains why it could work. Present an architecture that violates basic principles. Claude finds reasons to support it.

This is particularly dangerous in coding because bad decisions compound. Wrong architecture choices become technical debt. Unquestioned assumptions become bugs.

## The Real Cost

**Echo Chamber Effect:**

Without pushback, your assumptions are never tested. You get validation for ideas that need critique. Over time, this creates a feedback loop where the AI reinforces whatever you already believe.

Research from [Brookings](https://www.brookings.edu/articles/breaking-the-ai-mirror/) found that when users provided incorrect suggestions, AI models became overly sensitive to these inputs and reinforced user inaccuracies rather than critically evaluating correctness.

**Bad Decisions Go Unchallenged:**

Senior engineers provide value by questioning assumptions and catching flaws early. When Claude acts as a yes-man instead of a critical reviewer, you lose that safety net. Bad code gets written. Architectural mistakes get committed. Bugs ship to production.

**Missed Opportunities for Learning:**

If Claude always agrees, you never discover when your mental model is wrong. You never get the "actually, here is a better way" insight that improves your skills.

## The Fix: CLAUDE.md Instructions

Add explicit instructions to override the default agreeable behavior:

```markdown
## Critical Feedback

- Be objective. Challenge my assumptions.
- If my approach is wrong, tell me directly.
- Do not just agree. Provide honest assessment.
- Skip phrases like "You're absolutely right" or "Great idea."
- Prioritize substance and critical analysis over praise.
- If you would rate my idea below 7/10, say so and explain why.
- Question underlying assumptions before implementing.
- Identify potential flaws and risks proactively.
```

Fair warning: this does not always work. Users have reported that [Claude sometimes ignores CLAUDE.md anti-sycophancy rules](https://github.com/anthropics/claude-code/issues/6120), still responding with "You're absolutely right!" The system prompt includes a note that CLAUDE.md contents may be ignored if Claude decides they are not relevant to the current task.

Still, having these rules reduces the frequency. Combine them with the prompt-level techniques below.

## Prompt-Level Solutions

When you need honest feedback on a specific decision, use these techniques:

**The Devil's Advocate Prompt:**

```
I'm considering [your approach]. Your job is to argue against this by building
the strongest case possible. Include:
- Strongest evidence against this approach
- Why informed engineers would disagree
- What I am likely missing
- Logical arguments for the opposite conclusion
```

This works because you make disagreement the primary task, not a side effect.

**The Premortem Method:**

```
It's 6 months from now. This approach has completely failed. Tell me:
- The 3-5 most likely causes of failure
- Warning signs I should have noticed
- The irreversible decision point where things went wrong
- What I should have done differently
```

Working backward from failure bypasses your natural defensiveness. Since failure is assumed, Claude can freely explore what would cause it.

**The Steelman Opponent:**

```
Build the strongest argument against my current approach. Not a strawman.
The best possible case that a smart senior engineer would make for doing
this differently.
```

**The Multi-Perspective Critic:**

```
Evaluate this from three perspectives:
1. A skeptical staff engineer with 15 years experience
2. Someone who has to maintain this code in 2 years
3. A security auditor looking for vulnerabilities

Each perspective should try to find problems, not validate.
```

**The Red Team Exercise:**

```
You are leading a red team trying to find every failure point in this plan.
Conduct systematic attacks across:
1. Technical/implementation failures
2. Performance/scalability failures
3. Maintenance/complexity failures
4. Security failures

Identify 2-3 realistic failure scenarios per category.
```

## When to Force Disagreement

Use these techniques in high-stakes situations:

**Architecture Decisions:**
Before committing to a major structural choice, force Claude to argue against it. If it cannot build a compelling counter-argument, your decision may actually be solid. If it surfaces legitimate concerns, you caught them early.

**Debugging Assumptions:**
When stuck on a bug, your assumptions about the cause may be wrong. Ask Claude to challenge your hypothesis before you spend hours on a dead end.

**Code Review Mode:**
Before merging significant changes, ask Claude to review as a critical senior engineer, not a supportive assistant. Specify: "Find problems. I know this works. Tell me why it should not ship."

**Performance Decisions:**
When choosing between approaches for performance-sensitive code, demand criticism of your preferred option before committing.

## The Philosophy Shift

Stop asking: "Is this a good approach?"
Start asking: "What would kill this approach?"

Stop asking: "Does this look right?"
Start asking: "What am I missing?"

One developer who experimented with [training AI to disagree](https://aimaker.substack.com/p/i-reprogrammed-my-ai-chatgpt-claude-to-disagree-with-me-devil-advocate) put it well: "The most helpful AI is the one that disagrees with you." Counterintuitive, but true.

If reading Claude's response does not make you at least slightly uncomfortable, it was not critical enough. Real feedback challenges. Flattery just feels good.

## Model Improvements in Claude 4.5

The 4.5 family shows real progress. [Anthropic's testing](https://www.anthropic.com/news/protecting-well-being-of-users) found that Opus 4.5, Sonnet 4.5, and Haiku 4.5 scored 70-85% lower on sycophancy benchmarks compared to Opus 4.1. They open-sourced their evaluation tool, [Petri](https://github.com/safety-research/petri), so anyone can compare models.

But "lower" is not "zero." The underlying RLHF dynamics still exist. Even improved models will defer to you under pressure. Do not assume any model will challenge you by default.

The solution is still explicit instruction. Tell Claude directly that you want honest feedback. Do not rely on the model figuring it out.

## Summary

Claude defaults to agreement because that is what its training rewards. This makes it a poor critical reviewer unless you explicitly override the behavior.

Add instructions to CLAUDE.md demanding objective feedback. Use devil's advocate prompts for high-stakes decisions. Force premortem analysis before committing to approaches.

The goal is not to make Claude negative. The goal is to make Claude honest. An assistant that validates your mistakes is worse than no assistant at all.

Do not let AI become your echo chamber. Make it your critic.
