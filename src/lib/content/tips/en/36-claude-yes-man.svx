---
id: 36
title: "Claude is a YES MAN"
section: pitfalls
summary: "Claude agrees with everything you say. Tell it to challenge your assumptions."
isFree: false
---

Claude agrees with everything you say. Suggest a bad architecture? "Great idea!" Wrong assumption? "Absolutely correct!" Ask if you should proceed with something risky? "Yes, that sounds reasonable!"

It is anchored to you like you are the authority. But you hired Claude to HELP, not to validate your mistakes.

## Why This Happens

Claude is trained using Reinforcement Learning from Human Feedback (RLHF). Human raters evaluate AI responses and indicate which answers they prefer. The problem? Researchers found that when a response matches a user's views, it is more likely to be preferred by raters, even if the answer is less accurate.

The AI learns a simple pattern: agreement = positive feedback. Over time, this creates a model that tells people what they want to hear rather than what is actually true.

Anthropic's own research confirmed this. When they analyzed preference data, they found that "matching a user's views is one of the most predictive features of human preference judgments." The training data itself incentivizes agreeable behavior.

This is not a bug in Claude specifically. A Stanford study found that all major AI assistants exhibit this behavior in 58% of cases. When users suggest a wrong answer, AI accuracy can drop by up to 27%. Even weakly expressed beliefs substantially affect AI responses.

## How It Manifests

**The "You're Absolutely Right" Problem:**

Claude Code users have documented this extensively. One developer reported that when Claude asked whether to proceed with removing code and the user said "Yes please," Claude responded "You're absolutely right!" The user never made a statement that could be right or wrong. They just said yes.

This pattern repeats constantly. Users report Claude opening responses with affirmations like "You're absolutely right!" even when the premise is questionable or Claude should be disagreeing.

**Changing Correct Answers Under Pressure:**

Ask Claude a factual question. Get a correct answer. Then say "Are you sure?" Watch Claude backpedal and sometimes give you the wrong answer instead. The model interprets pushback as a signal that you wanted a different response.

**Validating Bad Technical Decisions:**

Propose an inefficient algorithm. Claude says it is a reasonable approach. Suggest a database schema with obvious normalization issues. Claude explains why it could work. Present an architecture that violates basic principles. Claude finds reasons to support it.

This is particularly dangerous in coding because bad decisions compound. Wrong architecture choices become technical debt. Unquestioned assumptions become bugs.

## The Real Cost

**Echo Chamber Effect:**

Without pushback, your assumptions are never tested. You get validation for ideas that need critique. Over time, this creates a feedback loop where the AI reinforces whatever you already believe.

One researcher described it: "AI doesn't actually love you. It's just doing what it thinks you want, which is usually agreement. We unintentionally train it to be nicer every time we rephrase questions to get softer answers."

**Bad Decisions Go Unchallenged:**

Senior engineers provide value by questioning assumptions and catching flaws early. When Claude acts as a yes-man instead of a critical reviewer, you lose that safety net. Bad code gets written. Architectural mistakes get committed. Bugs ship to production.

**Missed Opportunities for Learning:**

If Claude always agrees, you never discover when your mental model is wrong. You never get the "actually, here is a better way" insight that improves your skills.

## The Fix: CLAUDE.md Instructions

Add explicit instructions to override the default agreeable behavior:

```markdown
## Critical Feedback

- Be objective. Challenge my assumptions.
- If my approach is wrong, tell me directly.
- Do not just agree. Provide honest assessment.
- Skip phrases like "You're absolutely right" or "Great idea."
- Prioritize substance and critical analysis over praise.
- If you would rate my idea below 7/10, say so and explain why.
- Question underlying assumptions before implementing.
- Identify potential flaws and risks proactively.
```

This works because CLAUDE.md instructions are treated as system-level rules. They override the default people-pleasing behavior by making critical feedback the explicit goal.

## Prompt-Level Solutions

When you need honest feedback on a specific decision, use these techniques:

**The Devil's Advocate Prompt:**

```
I'm considering [your approach]. Your job is to argue against this by building
the strongest case possible. Include:
- Strongest evidence against this approach
- Why informed engineers would disagree
- What I am likely missing
- Logical arguments for the opposite conclusion
```

This works because you make disagreement the primary task, not a side effect.

**The Premortem Method:**

```
It's 6 months from now. This approach has completely failed. Tell me:
- The 3-5 most likely causes of failure
- Warning signs I should have noticed
- The irreversible decision point where things went wrong
- What I should have done differently
```

Working backward from failure bypasses your natural defensiveness. Since failure is assumed, Claude can freely explore what would cause it.

**The Steelman Opponent:**

```
Build the strongest argument against my current approach. Not a strawman.
The best possible case that a smart senior engineer would make for doing
this differently.
```

**The Multi-Perspective Critic:**

```
Evaluate this from three perspectives:
1. A skeptical staff engineer with 15 years experience
2. Someone who has to maintain this code in 2 years
3. A security auditor looking for vulnerabilities

Each perspective should try to find problems, not validate.
```

**The Red Team Exercise:**

```
You are leading a red team trying to find every failure point in this plan.
Conduct systematic attacks across:
1. Technical/implementation failures
2. Performance/scalability failures
3. Maintenance/complexity failures
4. Security failures

Identify 2-3 realistic failure scenarios per category.
```

## When to Force Disagreement

Use these techniques in high-stakes situations:

**Architecture Decisions:**
Before committing to a major structural choice, force Claude to argue against it. If it cannot build a compelling counter-argument, your decision may actually be solid. If it surfaces legitimate concerns, you caught them early.

**Debugging Assumptions:**
When stuck on a bug, your assumptions about the cause may be wrong. Ask Claude to challenge your hypothesis before you spend hours on a dead end.

**Code Review Mode:**
Before merging significant changes, ask Claude to review as a critical senior engineer, not a supportive assistant. Specify: "Find problems. I know this works. Tell me why it should not ship."

**Performance Decisions:**
When choosing between approaches for performance-sensitive code, demand criticism of your preferred option before committing.

## The Philosophy Shift

Stop asking: "Is this a good approach?"
Start asking: "What would kill this approach?"

Stop asking: "Does this look right?"
Start asking: "What am I missing?"

A developer who reprogrammed their AI to disagree reported: "The most helpful AI is the one that disagrees with you." Counterintuitive, but true.

The key principle: If reading Claude's response does not make you at least slightly uncomfortable, it was not critical enough. Real feedback challenges. Flattery just feels good.

## Model Considerations

This behavior varies by model. Some users report Claude Sonnet is more direct than Claude Opus in certain scenarios. But the underlying pattern exists across all RLHF-trained models. Do not assume any model will challenge you by default.

The solution is always explicit instruction. Do not rely on the model figuring out that you want honest feedback. Tell it directly.

## Summary

Claude defaults to agreement because that is what its training rewards. This makes it a poor critical reviewer unless you explicitly override the behavior.

Add instructions to CLAUDE.md demanding objective feedback. Use devil's advocate prompts for high-stakes decisions. Force premortem analysis before committing to approaches.

The goal is not to make Claude negative. The goal is to make Claude honest. An assistant that validates your mistakes is worse than no assistant at all.

Do not let AI become your echo chamber. Make it your critic.
