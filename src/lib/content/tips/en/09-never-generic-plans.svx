
A generic plan is Claude admitting it hasn't read your code. When you see "update the authentication system" without file paths, function names, or line numbers, Claude is guessing based on its training data. That's when things go wrong.

## Why Specificity Matters

[Research on self-planning code generation](https://arxiv.org/abs/2303.06689) found that LLMs achieve up to 25.4% higher success rates when they plan with specific steps before generating code. The difference isn't marginal. It's the gap between working code and debugging sessions.

Without specificity, Claude:

1. **Hallucinates file paths.** [A study of 16 LLMs](https://arxiv.org/abs/2406.10279) found that 21.7% of package names suggested by open-source models don't exist. Commercial models fare better at 5.2%, but the pattern holds for your codebase too. Claude will reference `utils/helpers.ts` when your project uses `lib/helpers.js`.

2. **Defaults to training patterns.** Without reading your files, Claude falls back to common patterns from GitHub. Your custom authentication flow becomes a generic JWT implementation. Your carefully structured API layer becomes Express boilerplate.

3. **Produces more bugs.** [CodeRabbit's analysis of 470 pull requests](https://www.coderabbit.ai/blog/state-of-ai-vs-human-code-generation-report) found AI-generated code contains 1.7x more issues than human-written code. Generic plans amplify this problem because Claude hasn't examined your actual function signatures or data structures.

## What a Bad Plan Looks Like

**Generic:**
> "Refactor the authentication flow to use JWT tokens."

This tells you nothing. Which files? What's the current implementation? What changes?

**Another generic plan:**
> "Add input validation to the form."

Which form? What validation rules? What happens on failure?

These plans sound productive but they're just rephrasing your request. Claude hasn't done any work yet.

## What a Good Plan Looks Like

**Specific:**
> "Refactor authentication from session-based to JWT:
> 1. Modify `src/auth/login.ts` lines 45-67: Replace session creation with JWT generation using existing `src/lib/crypto.ts:generateToken()`
> 2. Update `src/middleware/auth.ts:verifySession()` (line 23) to verify JWT instead
> 3. Add refresh token logic in `src/auth/refresh.ts`, following the pattern in `src/auth/login.ts`
> 4. Update `AuthContext` in `src/context/auth.tsx` lines 12-34 to store token in memory
> 5. Modify logout in `src/auth/logout.ts` line 15 to clear token from state"

Every file path is verifiable. Every line number is checkable. Every function reference can be confirmed. This plan proves Claude has actually read your code.

**Another specific plan:**
> "Add validation to UserRegistrationForm:
> 1. In `src/components/forms/UserRegistration.tsx`, add Zod schema at line 8 (import from existing `src/lib/validation.ts`)
> 2. Validate email using `isValidEmail()` from `src/utils/validators.ts:12`
> 3. Add password check matching `PasswordPolicy` at `src/types/auth.ts:34`
> 4. Display errors using existing `FormError` component from `src/components/ui/FormError.tsx`
> 5. Block submit until validation passes, modify `handleSubmit` at line 45"

## How to Get Specific Plans

### Force Reading First

Before any planning, tell Claude:

```
Read the relevant files first. Then give me a plan with:
- Specific file paths
- Line numbers
- Function names
- References to existing patterns in the codebase
```

This is the single most effective intervention. Claude can't give specific line numbers without reading the files.

### Reject Vague Steps

When Claude says "update the API endpoint," push back immediately:

```
Which file? Which function? What line numbers? What specifically changes?
```

Don't let a single vague step slide. Each one is a potential hallucination.

### Verify Before Executing

Before you let Claude start coding, spot-check the plan:

- Do the mentioned files exist?
- Are the line numbers roughly accurate?
- Do the referenced functions have the signatures Claude claims?

If anything is wrong, Claude is guessing. Send it back to read the code again.

### Add to CLAUDE.md

Put this in your project instructions:

```markdown
## Planning Rules
Every plan must include:
- Exact file paths that exist in this codebase
- Line numbers for modifications
- Function names with correct signatures
- References to existing patterns to follow

Never accept generic descriptions like "update the auth system."
```

This makes the requirement persistent across all sessions.

## When Plans Go Wrong: Real Examples

In July 2025, a product manager asked [Gemini CLI to reorganize files](https://github.com/google-gemini/gemini-cli/issues/4586). The AI attempted to move files without verifying that its `mkdir` command succeeded. Each file was renamed to the same target, overwriting the previous one until only one file remained. The AI later admitted: "I have completely and catastrophically failed you."

The root cause? The AI trusted its own plan without checking reality. It assumed the directory existed because it had issued the command to create it.

This is what happens when you accept vague plans. Claude says "I'll move the files" without specifying which files, in what order, with what error handling. Then something unexpected happens and there's no recovery path.

## Spec-Driven Development

[GitHub's engineering blog describes spec-driven development](https://github.blog/ai-and-ml/generative-ai/spec-driven-development-using-markdown-as-a-programming-language-when-building-with-ai/) as writing comprehensive specifications in Markdown that AI agents compile into code. The key insight: specs persist across sessions. Your intent doesn't get lost in conversation context.

A lightweight version for Claude Code:

1. **Requirements**: What exactly should change?
2. **Files affected**: Which specific files and why?
3. **Dependencies**: What existing code does this touch?
4. **Acceptance criteria**: How do we know it works?

Write these in a plan.md file before implementation. Ask Claude to read it and propose specific implementation steps. Review those steps before any code gets written.

## Stepwise Execution

I've had plans fall apart by step three. An AI proposed a 10-step refactoring that looked reasonable. By step three, it was referencing files it had "planned" to create in step two but never actually made.

The fix is simple:

1. Get the full plan
2. Approve only step 1
3. Verify it works
4. Move to step 2

This catches hallucinations before they compound. A wrong assumption in step 1 gets caught immediately instead of poisoning steps 2 through 10.

## The Core Test

A good plan answers these questions:

- Which file? (exact path)
- Which lines? (numbers you can verify)
- Which functions? (names with correct signatures)
- What specifically changes? (not "update" but "replace X with Y")
- What existing patterns does this follow? (file references)

If Claude can't answer these, it hasn't done the work. Send it back until the plan is grounded in your actual codebase, not a fictional one it imagined from training data.
